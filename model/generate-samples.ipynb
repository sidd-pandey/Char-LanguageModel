{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"generate-samples.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"mq4VDczJL0TD","colab_type":"text"},"cell_type":"markdown","source":["Use below import if using Google Colab, to mount the drive"]},{"metadata":{"id":"_RVN14ylo02O","colab_type":"code","outputId":"9d967a31-6a95-4376-d9a3-4449e5c56e93","executionInfo":{"status":"ok","timestamp":1554641061395,"user_tz":-480,"elapsed":1067,"user":{"displayName":"Siddharth Pandey","photoUrl":"","userId":"17944738894945738752"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":63,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"metadata":{"id":"eESrCNcdMAr5","colab_type":"text"},"cell_type":"markdown","source":["**Required imports**"]},{"metadata":{"id":"Kelos8oHo7Dd","colab_type":"code","colab":{}},"cell_type":"code","source":["import json\n","import os\n","import numpy as np\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","from keras.utils import np_utils\n","import pandas as pd\n","\n","import keras\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Dropout\n","from keras.layers import LSTM\n","from keras.layers import Conv1D, MaxPool1D\n","from keras.callbacks import ModelCheckpoint\n","from keras.models import load_model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"omBBHsG2MG1V","colab_type":"text"},"cell_type":"markdown","source":["Set the root path accordinly. Root path cannot be realtive, must be absolute i.e. beginning with C:, /usr/ etc."]},{"metadata":{"id":"dqkOZo22o-Sv","colab_type":"code","colab":{}},"cell_type":"code","source":["root_path = \"gdrive/My Drive/colab_notebooks/LM-SampleGeneration\""],"execution_count":0,"outputs":[]},{"metadata":{"id":"8xvPLblMMQYi","colab_type":"text"},"cell_type":"markdown","source":["Read the files, make sure that they are in path below."]},{"metadata":{"id":"ixAIhZoVpMVG","colab_type":"code","colab":{}},"cell_type":"code","source":["with open(root_path + \"/data/all_story_cleaned.txt\", encoding = \"utf-8\") as f:\n","  raw_text = f.read()\n","  \n","with open(root_path + \"/data/vocab_clenaed-50.json\", encoding=\"utf-8\") as f:\n","  json_dump = json.load(f)\n","  char_to_int = json_dump[\"char_to_int\"]\n","  int_to_char = json_dump[\"int_to_char\"]\n","  int_to_char = {int(k):int_to_char[k] for k in int_to_char}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0gT6FYRBMZS8","colab_type":"text"},"cell_type":"markdown","source":["Global variables to define properties of the experiment"]},{"metadata":{"id":"HzYGgjARqvxG","colab_type":"code","colab":{}},"cell_type":"code","source":["n_vocab = len(int_to_char)\n","max_len = 50"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FzJks29HMtH7","colab_type":"text"},"cell_type":"markdown","source":["Returns predicted character for the given probabilty distribution. This method remain same irrespective of model used for prediction.  <br>\n","Argument ***pred:*** the probabiliteis score the output predicted. <br>\n","Argument ***temperature:*** defines the extent tne neural network can get adventurous in predicting the next character. Higher the temperature value, more chances of infrequent character getting selected. <br>\n","**Returns:** integer id of the character selected for prediction"]},{"metadata":{"id":"iy8eoms_xSij","colab_type":"text"},"cell_type":"markdown","source":["Nice explanation for temepraure based sampling <br>\n","https://stats.stackexchange.com/questions/255223/the-effect-of-temperature-in-temperature-sampling"]},{"metadata":{"id":"QJxAzlTe1FFm","colab_type":"code","colab":{}},"cell_type":"code","source":["def sample(preds, temperature=1.0):\n","    # helper function to sample an index from a probability array\n","    preds = np.asarray(preds).astype('float64')\n","    preds = np.log(preds) / temperature\n","    exp_preds = np.exp(preds)\n","    preds = exp_preds / np.sum(exp_preds)\n","    # select a character based on probability\n","    probas = np.random.multinomial(1, preds, 1)\n","    return np.argmax(probas)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gPsG30gTOmQd","colab_type":"text"},"cell_type":"markdown","source":["Method to generate samples for a given model and text. <br>\n","Argument ***model:*** the model over which prediction will be called, cannot be **None**. <br>\n","Argument ***text:*** the text to be used  as seed for generating sample. **Optional**, in case not provided a random seed is selected from raw text. <br>\n","Argument ***temperatures:*** the temperature/diversity for generated text. It also governs the number of generated text per method call. **Optional**, if not provided 8 samples generated for mix of values.<br>\n","**Returns** tuple object containing seed and corresponding generated text."]},{"metadata":{"colab_type":"code","id":"DTZFggvM5Zvt","colab":{}},"cell_type":"code","source":["def generate_sample(model, model_desc, text=None, \n","                    temperatures=[0.7, 0.7, 0.6, 0.6,0.5, 0.5, 0.4, 0.4]):\n","  \n","  samples = []\n","  \n","  if text is not None:\n","    custom = True\n","    original_text = text\n","  else: custom = False\n","  \n","  # run for each temperature value\n","  for diversity in temperatures:\n","    \n","    # if custome text is not provided\n","    if not custom:\n","      # select a random start point\n","      start = np.random.randint(len(raw_text)-(max_len + 1))\n","      # select portion of text till max length\n","      text = raw_text [start : start + max_len]\n","    else:\n","      # if custom text is provided, use lower case chars\n","      # make that given text has character present in vocab (50 chars)\n","      # pad it with space, if custome text is shorter than max lenght\n","      text = original_text\n","      text = \" \"*max_len + text.lower()\n","      # slice last max len chars\n","      text = text[-max_len:]\n","    \n","    # set the seed\n","    seed = text\n","    # tokenize the text\n","    tokenized_text = [char_to_int[c] for c in text]\n","    \n","    # conver to write shape: (1, max_len, n_vocab)\n","    x_seq = np_utils.to_categorical(tokenized_text, num_classes=n_vocab)\n","    # add batch axis as it was missing\n","    x_seq = np.expand_dims(x_seq, axis = 0)\n","    \n","    # generate 200 chars\n","    for i in range(200):\n","      \n","      # get the predictions orignal shape of prediction is (1, max_vocab), take first test sample from batch axis. final shape (50, )\n","      preds = model.predict(x_seq, verbose=0)[0]\n","      \n","      # use temperature based sampling\n","      next_index = sample(preds, diversity)\n","      # untokenize the index to get the character\n","      next_char = int_to_char[next_index]\n","      # append the char\n","      text = text + next_char\n","      # prepare a new array for prediction. shape (1, 1, 1).\n","      new_arr = np_utils.to_categorical(next_index, num_classes=n_vocab).reshape(1, 1, -1)\n","      # append it to current X, and take all elements other than first. (sliding window approach)\n","      # use this for prediction next iteration\n","      x_seq = np.concatenate([x_seq, new_arr], axis = 1)[:, 1:, :]\n","      \n","    # append to array to return\n","    samples.append((model_desc, diversity, seed, text))\n","    print(\"generated for temp:\", diversity)\n","  \n","  # return the all the samples\n","  return samples"],"execution_count":0,"outputs":[]},{"metadata":{"id":"P0zZJzHUPpGT","colab_type":"text"},"cell_type":"markdown","source":["Set the model path for which text needs to be generated accordingly"]},{"metadata":{"id":"4AhPceEFjCKl","colab_type":"code","colab":{}},"cell_type":"code","source":["two_layer_lstm = [root_path + \"/models/best_model-2layer-512-256.h5\", \"2 Layer LSTM\"]\n","three_layer_lstm = [root_path + \"/models/best_model-3layer-512-256-256.h5\", \"3 Layer LSTM\"]\n","two_layer_cnn_lstm = [root_path + \"/models/best_model-2cnn-2layer-1stride-50len.h5\", \"2 Layer LSTM with Char-CNN\"]\n","pair_embedding = [root_path + \"/models/best_model-pair-embedding.h5\", \"3 Layer LSTM with Pair Embeddings\"]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xBgALbA_tGRf","colab_type":"text"},"cell_type":"markdown","source":["**Generating a few samples for fun**"]},{"metadata":{"id":"jTwFNnLItLAB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":207},"outputId":"d90e982f-fc1c-49d3-97ef-15c9f237d919","executionInfo":{"status":"ok","timestamp":1554642129060,"user_tz":-480,"elapsed":93788,"user":{"displayName":"Siddharth Pandey","photoUrl":"","userId":"17944738894945738752"}}},"cell_type":"code","source":["good_model = load_model(three_layer_lstm[0])\n","\n","# do not pass text if want try random generation.\n","# char generation takes a few minutes.\n","generated_text = generate_sample(good_model, three_layer_lstm[1], text = \"you shall know a word by \", temperatures=[0.6, 0.6])\n","for text_tuple in generated_text:\n","  print(\"seed:\")\n","  print(text_tuple[2])\n","  print(\"generated text:\")\n","  print(text_tuple[3])"],"execution_count":79,"outputs":[{"output_type":"stream","text":["generated for temp: 0.6\n","generated for temp: 0.6\n","seed:\n","                         you shall know a word by \n","generated text:\n","                         you shall know a word by the latter world, or i do not come in to see him in the first tage to the promise of the unportrait days of the content in the same distance, and has properted by her shoes, and in the new day of by t\n","seed:\n","                         you shall know a word by \n","generated text:\n","                         you shall know a word by the servant, my love, we looked at me and find for the practice of means in the barnacle, with a deserted great lian, and to tell me what i should make the stranger when you are satisfied in the mains\n"],"name":"stdout"}]},{"metadata":{"id":"x0-zEiH75MbC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":207},"outputId":"2fd56fd8-8c66-4bb1-994c-5f2b11e15c76","executionInfo":{"status":"ok","timestamp":1554642220739,"user_tz":-480,"elapsed":48609,"user":{"displayName":"Siddharth Pandey","photoUrl":"","userId":"17944738894945738752"}}},"cell_type":"code","source":["generate_sample(good_model, three_layer_lstm[1], temperatures=[0.6, 0.6])"],"execution_count":80,"outputs":[{"output_type":"stream","text":["generated for temp: 0.6\n","generated for temp: 0.6\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[('3 Layer LSTM',\n","  0.6,\n","  'board, to make my supper on when i came back at ni',\n","  'board, to make my supper on when i came back at night. what i find them being one of the houses, and there had felt an appearance of the boy, and before me who had come upon him to the proposal of his common, had come to him to be revariously and mos'),\n"," ('3 Layer LSTM',\n","  0.6,\n","  ' kit. ‘but i am to give it to himself, if you plea',\n","  ' kit. ‘but i am to give it to himself, if you please you know the son was to be a destruction of the table, he had the bell and a subject of which the sense of little children did i couldn’t have no servant and her dear gentleman and mr. nickleby in ')]"]},"metadata":{"tags":[]},"execution_count":80}]},{"metadata":{"id":"x7t3g0RDdzJN","colab_type":"text"},"cell_type":"markdown","source":["**Batch Generation for Human Evaluation**"]},{"metadata":{"id":"YtqIqVtqZvgE","colab_type":"code","colab":{}},"cell_type":"code","source":["all_models = [two_layer_lstm, three_layer_lstm , two_layer_cnn_lstm, pair_embedding]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"aq3FQXzJcNZx","colab_type":"text"},"cell_type":"markdown","source":["For each model generate 20 pair of text and save it as text file for human evaluation"]},{"metadata":{"id":"OUfuQzhaQE92","colab_type":"code","outputId":"fb62ca5e-a272-417c-e644-df808b416614","executionInfo":{"status":"ok","timestamp":1554626390602,"user_tz":-480,"elapsed":2074304,"user":{"displayName":"Siddharth Pandey","photoUrl":"","userId":"17944738894945738752"}},"colab":{"base_uri":"https://localhost:8080/","height":2346}},"cell_type":"code","source":["gen_samples = []\n","for model_item in all_models:\n","  model_path = model_item[0]\n","  model_desc = model_item[1]\n","  \n","  model = load_model(model_path)\n","  \n","  batches = 5\n","  print(\"generating \" + str(batches) + \" batches for model:\", model_desc)\n","  \n","  for num in range(batches):\n","    print(\"started for batch:\", num)\n","    gen_samples = gen_samples + generate_sample(model, model_desc, \n","                                temperatures=[0.7, 0.6, 0.6, 0.5, 0.5])\n","    \n","\n","df = pd.DataFrame(gen_samples, columns = [\"model\", \"temperature\", \"seed\", \"generated text\"])\n","df.to_csv(root_path+\"/generated_sample-2.csv\", sep=\"#\", index=False)\n","df.head()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["generating 5 batches for model: 2 Layer LSTM\n","started for batch: 0\n","generated for temp: 0.7\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n","  after removing the cwd from sys.path.\n"],"name":"stderr"},{"output_type":"stream","text":["generated for temp: 0.6\n","generated for temp: 0.6\n","generated for temp: 0.5\n","generated for temp: 0.5\n","started for batch: 1\n","generated for temp: 0.7\n","generated for temp: 0.6\n","generated for temp: 0.6\n","generated for temp: 0.5\n","generated for temp: 0.5\n","started for batch: 2\n","generated for temp: 0.7\n","generated for temp: 0.6\n","generated for temp: 0.6\n","generated for temp: 0.5\n","generated for temp: 0.5\n","started for batch: 3\n","generated for temp: 0.7\n","generated for temp: 0.6\n","generated for temp: 0.6\n","generated for temp: 0.5\n","generated for temp: 0.5\n","started for batch: 4\n","generated for temp: 0.7\n","generated for temp: 0.6\n","generated for temp: 0.6\n","generated for temp: 0.5\n","generated for temp: 0.5\n","generating 5 batches for model: 3 Layer LSTM\n","started for batch: 0\n","generated for temp: 0.7\n","generated for temp: 0.6\n","generated for temp: 0.6\n","generated for temp: 0.5\n","generated for temp: 0.5\n","started for batch: 1\n","generated for temp: 0.7\n","generated for temp: 0.6\n","generated for temp: 0.6\n","generated for temp: 0.5\n","generated for temp: 0.5\n","started for batch: 2\n","generated for temp: 0.7\n","generated for temp: 0.6\n","generated for temp: 0.6\n","generated for temp: 0.5\n","generated for temp: 0.5\n","started for batch: 3\n","generated for temp: 0.7\n","generated for temp: 0.6\n","generated for temp: 0.6\n","generated for temp: 0.5\n","generated for temp: 0.5\n","started for batch: 4\n","generated for temp: 0.7\n","generated for temp: 0.6\n","generated for temp: 0.6\n","generated for temp: 0.5\n","generated for temp: 0.5\n","generating 5 batches for model: 2 Layer LSTM with Char-CNN\n","started for batch: 0\n","generated for temp: 0.7\n","generated for temp: 0.6\n","generated for temp: 0.6\n","generated for temp: 0.5\n","generated for temp: 0.5\n","started for batch: 1\n","generated for temp: 0.7\n","generated for temp: 0.6\n","generated for temp: 0.6\n","generated for temp: 0.5\n","generated for temp: 0.5\n","started for batch: 2\n","generated for temp: 0.7\n","generated for temp: 0.6\n","generated for temp: 0.6\n","generated for temp: 0.5\n","generated for temp: 0.5\n","started for batch: 3\n","generated for temp: 0.7\n","generated for temp: 0.6\n","generated for temp: 0.6\n","generated for temp: 0.5\n","generated for temp: 0.5\n","started for batch: 4\n","generated for temp: 0.7\n","generated for temp: 0.6\n","generated for temp: 0.6\n","generated for temp: 0.5\n","generated for temp: 0.5\n","generating 5 batches for model: 3 Layer LSTM with Pair Embeddings\n","started for batch: 0\n","generated for temp: 0.7\n","generated for temp: 0.6\n","generated for temp: 0.6\n","generated for temp: 0.5\n","generated for temp: 0.5\n","started for batch: 1\n","generated for temp: 0.7\n","generated for temp: 0.6\n","generated for temp: 0.6\n","generated for temp: 0.5\n","generated for temp: 0.5\n","started for batch: 2\n","generated for temp: 0.7\n","generated for temp: 0.6\n","generated for temp: 0.6\n","generated for temp: 0.5\n","generated for temp: 0.5\n","started for batch: 3\n","generated for temp: 0.7\n","generated for temp: 0.6\n","generated for temp: 0.6\n","generated for temp: 0.5\n","generated for temp: 0.5\n","started for batch: 4\n","generated for temp: 0.7\n","generated for temp: 0.6\n","generated for temp: 0.6\n","generated for temp: 0.5\n","generated for temp: 0.5\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model</th>\n","      <th>temperature</th>\n","      <th>seed</th>\n","      <th>generated text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2 Layer LSTM</td>\n","      <td>0.7</td>\n","      <td>ht and to suffer her to talk on, as it was evi...</td>\n","      <td>ht and to suffer her to talk on, as it was evi...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2 Layer LSTM</td>\n","      <td>0.6</td>\n","      <td>d, and i can you take it for in this man, of c...</td>\n","      <td>d, and i can you take it for in this man, of c...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2 Layer LSTM</td>\n","      <td>0.6</td>\n","      <td>d in the street, mr. jarndyce as she said to b...</td>\n","      <td>d in the street, mr. jarndyce as she said to b...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2 Layer LSTM</td>\n","      <td>0.5</td>\n","      <td>ld not have the little moment to every walk, a...</td>\n","      <td>ld not have the little moment to every walk, a...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2 Layer LSTM</td>\n","      <td>0.5</td>\n","      <td>n of end in the next side of the most self-str...</td>\n","      <td>n of end in the next side of the most self-str...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          model  temperature  \\\n","0  2 Layer LSTM          0.7   \n","1  2 Layer LSTM          0.6   \n","2  2 Layer LSTM          0.6   \n","3  2 Layer LSTM          0.5   \n","4  2 Layer LSTM          0.5   \n","\n","                                                seed  \\\n","0  ht and to suffer her to talk on, as it was evi...   \n","1  d, and i can you take it for in this man, of c...   \n","2  d in the street, mr. jarndyce as she said to b...   \n","3  ld not have the little moment to every walk, a...   \n","4  n of end in the next side of the most self-str...   \n","\n","                                      generated text  \n","0  ht and to suffer her to talk on, as it was evi...  \n","1  d, and i can you take it for in this man, of c...  \n","2  d in the street, mr. jarndyce as she said to b...  \n","3  ld not have the little moment to every walk, a...  \n","4  n of end in the next side of the most self-str...  "]},"metadata":{"tags":[]},"execution_count":62}]}]}